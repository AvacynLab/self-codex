----------
Voici ta **liste de t√¢ches exhaustive et hi√©rarchis√©e, √† cocher**, **adress√©e directement √† toi, Agent**, pour **cr√©er, int√©grer et valider** le moteur de recherche LLM (SearxNG + unstructured.io) dans le d√©p√¥t actuel (`self-codex-main`). Je m‚Äôappuie sur l‚Äôarborescence r√©elle du repo et sur nos sp√©cifications pr√©c√©dentes.
Objectif : livrer un module `src/search/**` isol√©, des tools MCP `search.*`, une infra **self-host** SearxNG + Unstructured (docker), l‚Äôingestion dans **KnowledgeGraph** et **VectorMemory**, l‚Äôobservabilit√©, et une batterie de **tests** (unit/int/e2e) en conformit√© avec la toolchain (Node ‚â• 20, ESM, TS strict, exactOptionalPropertyTypes, Mocha+tsx).

---

## üéØ Brief (lis √ßa d‚Äôabord)

**Objectifs attendus (r√©sum√©)**
Tu vas :

1. Impl√©menter le **module `src/search/**`** (client SearxNG, t√©l√©chargement, extraction via unstructured, normalisation, ingestion KG + VectorMemory, m√©triques).
2. Exposer **3 tools MCP** : `search.run`, `search.index`, `search.status`, et les **enregistrer** via `src/mcp/registry.ts`.
3. Ajouter une **infra self-host** : `docker/docker-compose.search.yml` (services `searxng`, `unstructured`, `server`) + `docker/searxng/settings.yml`.
4. √âtendre l‚Äô**observabilit√©** : nouveaux events/m√©triques visibles dans le **dashboard**.
5. √âcrire des **tests unitaires, d‚Äôint√©gration, et E2E** (mocks HTTP et avec containers r√©els).
6. Mettre √† jour les **docs** et **.env.example**.

**Contraintes de qualit√© / build**

* **TypeScript strict** + `exactOptionalPropertyTypes: true` (d√©j√† actif dans `tsconfig.json`) : ne jamais exposer `undefined` dans les sorties publiques (pr√©f√©rer **omission** des cl√©s).
* Respect des **budgets/timeout** outill√©s (c√¥t√© controller/registry), **idempotence** HTTP, **rate limiting** (si expos√© via HTTP).
* **Imports Node** en `node:` pour ESM (conventions existantes).
* **Tests** : Mocha + tsx (`npm run test`, `npm run test:int`, `npm run test:e2e`). Viser **‚â•90%** de couverture sur `src/search/**` et **‚â•85%** global.
* **Observabilit√©** : tracer les √©tapes critiques (Searx, fetch, extract, ingest) et √©mettre des events **stables** et diffables.

---

## A) Infrastructure : SearxNG & Unstructured (self-host)

> Nouveaux fichiers (dans `docker/`), adoption CI/dev.

* [x] **Cr√©er** `docker/docker-compose.search.yml`

  * [x] Service `searxng` (image `searxng/searxng:latest`)

    * [x] Ports : `127.0.0.1:8080:8080`
    * [x] Volume : `./searxng/settings.yml:/etc/searxng/settings.yml:ro`
    * [x] Healthcheck HTTP (ex: `/healthz` ou page d‚Äôaccueil)
  * [x] Service `unstructured` (image `quay.io/unstructured-io/unstructured-api:latest`)

    * [x] Ports : `127.0.0.1:8000:8000`
    * [x] Healthcheck minimal (POST `/general/v0/general` ping)
  * [x] Service `server` (build depuis Dockerfile du projet)

    * [x] `depends_on` avec `condition: service_healthy` pour `searxng` et `unstructured`
    * [x] Variables d‚Äôenv (cf. `.env.example` √† compl√©ter)
  * [x] **R√©seau** d√©di√© (bridge interne) `search_net`
  * [x] **Ressources** : limites CPU/RAM raisonnables (√©viter OOM en CI)

* [x] **Cr√©er** `docker/searxng/settings.yml`

  * [x] Configurer **engines** pertinents (p.ex. `bing`, `ddg`, `wikipedia`, `arxiv`, `github`, ‚Ä¶)
  * [x] Configurer **categories** (au minimum : `general,news,images,files`)
  * [x] `safe_search` selon besoin (0/1), `retries`, `result_proxy` si requis
  * [x] Cl√©s/secret c√¥t√© prod via secret/env (ne rien commiter de sensible)

* [x] **Mettre √† jour** `env/.env.example`

  * [x] `SEARCH_SEARX_BASE_URL=http://searxng:8080`
  * [x] `SEARCH_SEARX_API_PATH=/search`
  * [x] `SEARCH_SEARX_TIMEOUT_MS=15000`
  * [x] `SEARCH_SEARX_ENGINES=bing,ddg,wikipedia,arxiv,github`
  * [x] `SEARCH_SEARX_CATEGORIES=general,news,images,files`
  * [x] `UNSTRUCTURED_BASE_URL=http://unstructured:8000`
  * [x] `UNSTRUCTURED_TIMEOUT_MS=30000`
  * [x] `UNSTRUCTURED_STRATEGY=hi_res`
  * [x] `SEARCH_FETCH_TIMEOUT_MS=20000`
  * [x] `SEARCH_FETCH_MAX_BYTES=15000000`
  * [x] `SEARCH_FETCH_UA=CodexSearchBot/1.0`
  * [x] `SEARCH_INJECT_GRAPH=true`
  * [x] `SEARCH_INJECT_VECTOR=true`

---

## B) Nouveau module isol√© : `src/search/**`

> Tous les fichiers suivants sont **nouveaux** et doivent respecter les conventions ESM/TS strictes du repo.

### B1. Types & Config

* [x] **Cr√©er** `src/search/types.ts`

  * [x] `SearxResult` : `{ url:string; title?:string; snippet?:string; engine?:string; category?:string; publishedAt?:string; mime?:string }`
  * [x] `RawFetched` : `{ url; status; headers; contentType?; body:Buffer; fetchedAt }`
  * [x] `StructuredSegment` : `{ kind:'title'|'paragraph'|'list'|'table'|'figure'|'caption'|'code'|'meta'; text?; page?; bbox?; meta? }`
  * [x] `StructuredDocument` : `{ id; url; source; contentType; language?; title?; publishedAt?; fetchedAt; metadata; segments; provenance:{ engines?; categories?; searxQuery? } }`
  * [x] **Aucun `undefined` export√©** dans les objets publics (omets les cl√©s absentes)

* [x] **Cr√©er** `src/search/config.ts`

  * [x] Lire/normaliser env ‚Üí `SearchConfig` (searx, unstructured, fetch, pipeline)
  * [x] Parser `engines/categories` CSV en `string[]` (ignore vides)

### B2. Clients & √âtapes du pipeline

* [x] **Cr√©er** `src/search/searxClient.ts`

  * [x] `searxQuery(q,{categories,engines,count}) => Promise<SearxResult[]>`
  * [x] Requ√™te `format=json`, zod pour valider `results[]`, timeouts, bearer optionnel, 1‚Äì2 retries

* [x] **Cr√©er** `src/search/downloader.ts`

  * [x] `fetchUrl(url) => RawFetched` (follow, `content-type` sans charset, clamp taille max, erreurs 4xx/5xx)
  * [x] `computeDocId(url, headers) => sha256(url+etag+last-modified)`
  * [x] Respect `SEARCH_FETCH_UA`, option **robots.txt** si tu ajoutes le contr√¥le

* [x] **Cr√©er** `src/search/extractor.ts`

  * [x] `extractWithUnstructured(raw, {url,title?,engines?,categories?}) => StructuredDocument`
  * [x] Appel API Unstructured `/general/v0/general` (multipart), `strategy` configurable
  * [x] Mapper √©l√©ments ‚Üí `StructuredSegment` (page, bbox, meta)
  * [x] D√©tection **langue** (lib simple), propagation `contentType`
  * [x] **Ne pas** exposer de `undefined` : champs optionnels omis

* [x] **Cr√©er** `src/search/normalizer.ts`

  * [x] `finalizeDocId(doc)` : remplace `id` par `computeDocId`
  * [x] `deduplicateSegments(doc)` : d√©duplique via `(kind|text.trim)`

### B3. Ingestion vers KG + VectorMemory

* [x] **Cr√©er** `src/search/ingest/toKnowledgeGraph.ts`

  * [x] Utiliser la **classe** `KnowledgeGraph` existante (`src/knowledge/knowledgeGraph.ts`) ‚Üí m√©thode **`insert`**
  * [x] Ins√©rer le n≈ìud Document (type `Document`, `dc:title`, `dc:source` (URL), `dc:language`) avec **provenance** (sourceUrl, fetchedAt)
  * [x] MVP : ajouter des `mentions` bas√©es sur termes cl√©s extraits (stopwords basiques)
  * [x] Pr√©voir un hook ult√©rieur NER/LLM (non activ√© par d√©faut)

* [x] **Cr√©er** `src/search/ingest/toVectorStore.ts`

  * [x] Utiliser `VectorMemoryIndex` (`src/memory/vector.ts`) ‚Üí m√©thode **`upsert`**
  * [x] S√©lectionner segments `paragraph|title|list`, normaliser, chunker si util existant (sinon na√Øf)
  * [x] `metadata` : `{ docId, url, title, language, fetchedAt }`, tags vides au d√©part
  * [x] **Pas** d‚Äôacc√®s √† des embeddings externes : on reste sur l‚Äôindex texte‚ÜíTF-IDF du repo

### B4. Orchestrateur de pipeline + m√©triques

* [x] **Cr√©er** `src/search/pipeline.ts`

  * [x] `runSearchJob({ query, categories?, engines?, maxResults?, fetchContent?, injectGraph?, injectVector? })`
  * [x] √âtapes : `searxQuery` ‚Üí pick top K ‚Üí `fetchUrl` ‚Üí `extractWithUnstructured` ‚Üí `finalizeDocId` ‚Üí `deduplicateSegments` ‚Üí ingestion KG + Vector
  * [x] **√âmettre des events** via `EventStore` (voir section D)
  * [x] G√©rer les erreurs URL individuellement (continuer le job)
  * [x] Retourner `{ results:[‚Ä¶], docs:[{id,url,title,language}] }`

* [x] **Cr√©er** `src/search/metrics.ts`

  * [x] Int√©gration `infra/tracing.ts` pour mesurer latences p50/p95/p99 des fonctions : `searxQuery`, `fetchUrl`, `extractWithUnstructured`, `ingestToGraph`, `ingestToVector`

* [x] **Cr√©er** `src/search/index.ts`

  * [x] Export centralis√© des symboles du module

* [x] **(Optionnel recommand√©)** `src/search/cache/contentCache.ts`

  * [x] Cache disque par URL + (etag|last-modified), TTL par domaine, backoff sur 4xx/5xx

---

## C) Tools MCP : exposition contr√¥l√©e

> S‚Äôinspirer du style des tools existants dans `src/tools/**`. Utiliser zod, budgets, `buildToolSuccessResult`.

* [x] **Cr√©er** `src/tools/search_run.ts`

  * [x] Entr√©e (zod) : `{ query:string; categories?:string[]; engines?:string[]; maxResults?:number(1..50); fetchContent?:boolean; injectGraph?:boolean; injectVector?:boolean }`
  * [x] Budgets init : `{ timeMs: 60_000, toolCalls: 4, bytesOut: 512_000 }` (ajuste si besoin)
  * [x] Appeler `runSearchJob` et **retourner** `{ ok:true, count, docs:[{id,url,title,language}] }` via helper de succ√®s
  * [x] **Ne pas** renvoyer de blobs ; **pas** d‚Äô`undefined`

* [x] **Cr√©er** `src/tools/search_index.ts`

  * [x] Entr√©e : `{ url:string | string[]; title?:string; injectGraph?:boolean; injectVector?:boolean }`
  * [x] Boucle `fetch ‚Üí extract ‚Üí normalize ‚Üí ingest`
  * [x] Retour `{ ok:true, docs:[{id,url,title}] }`

* [x] **Cr√©er** `src/tools/search_status.ts`

  * [x] Si persistance de jobs **non** mise en place : renvoyer proprement ‚Äúnon impl√©ment√©‚Äù (code/tool result standard)

* [x] **Modifier** `src/mcp/registry.ts`

  * [x] **Enregistrer** les 3 tools (`search.run`, `search.index`, `search.status`) dans le pack appropri√© (p.ex. `authoring` ou `ops`)
  * [x] Ajouter **tags** (`search`,`web`,`ingest`,`rag`) et **budgets**
  * [x] Ajouter un **exemple d‚Äôappel** dans la description (coh√©rent avec les autres tools)

---

## D) Int√©gration orchestrateur & observabilit√©

* [x] **√âv√©nements EventStore** : **ne pas inventer un nouveau `EventKind`** si tu ne touches pas le type union

  * [x] Utiliser `EventKind` existants (p.ex. `"INFO"` pour les √©tapes Search) et inclure un `payload` structur√© :

    * [x] `search:job_started` ‚Üí `{ query, categories, engines, maxResults }`
    * [x] `search:doc_ingested` ‚Üí `{ docId, url, injectGraph, injectVector }`
    * [x] `search:error` ‚Üí `{ url, error }`
    * [x] `search:job_completed` ‚Üí `{ query, tookMs, docCount }`
  * [x] **Alternative (option ‚Äúplus propre‚Äù)** : √©tendre l‚Äôunion `EventKind` de `src/eventStore.ts` pour ajouter `"SEARCH"` et router ces events sous ce kind (exige update des types et dashboard). Si tu fais √ßa :

    * [x] Mettre √† jour toutes les occurrences de l‚Äôunion dans le repo (compilateur t‚Äôaidera)
    * [x] Adapter `monitor/dashboard.ts` (voir ci-dessous)

* [x] **Modifier** `src/monitor/dashboard.ts`

  * [x] Ajouter un **panneau Search** (HTML) et un **endpoint JSON** r√©capitulant :

    * [x] `jobs` r√©cents, `docs/min`, `errors by type`, latence moyenne par `contentType`
  * [x] Brancher la **SSE** si n√©cessaire (en t‚Äôinspirant des autres panneaux)
  * [x] **Ne pas** casser les pages existantes (garde la navigation)

* [x] **Modifier** `src/runtime/*` si n√©cessaire pour injecter des d√©pendances (logger/tracing) vers `src/search/pipeline.ts` (garde la composition root **pure**)

* [x] **HTTP idempotence/rate-limit** (si tools expos√©s via HTTP)

  * [x] Dans `src/httpServer.ts` et `src/orchestrator/controller.ts`, v√©rifier que l‚Äô**idempotency cache key** couvre `search.run` (cl√© = hash des arguments pertinents)
  * [x] Ajouter un **rate-limit** sp√©cifique pour `search.*` si la conf le permet

---

## E) S√©curit√©, conformit√© & robustesse

* [x] **User-Agent** d√©di√© (`SEARCH_FETCH_UA`), **timeouts** et **limites de taille** stricts
* [x] **robots.txt** : si tu ajoutes le respect des robots, le rendre **activable par env**, et logguer une alerte si bloqu√©
* [x] **Redaction** des headers sensibles dans les logs (`logger` central)
* [x] **Provenance** : toujours renseigner `sourceUrl`, `fetchedAt`, + provenance Unstructured si utile
* [x] **Licences engines SearxNG** : commenter/d√©sactiver ceux √† risque dans `settings.yml`

---

## F) Tests (unit, int√©gration, E2E)

> Respecter la structure existante : **Mocha + tsx**, fichiers `*.test.ts`, r√©pertoires sous `tests/`. Viser ‚â•90% de cov. `src/search/**`.

### F1. Unitaires (nouveaux)

* [x] **Cr√©er** `tests/unit/search/searxClient.test.ts`

  * [x] Mocker r√©ponses SearxNG (OK / sch√©ma invalide / 500 / timeout)
  * [x] V√©rifier parsing zod et mapping ‚Üí `SearxResult[]`
  * [x] V√©rifier params (`q`, `categories`, `engines`, `count`)

* [x] **Cr√©er** `tests/unit/search/downloader.test.ts`

  * [x] Mocker HTTP : redirects, 404/500, `content-type` vari√©s, clamp taille
  * [x] Tester `computeDocId` (variation ETag/Last-Modified)

* [x] **Cr√©er** `tests/unit/search/extractor.test.ts`

  * [x] Mocker Unstructured (`/general/v0/general`) avec jeux d‚Äô√©l√©ments (title/paragraph/list/table/figure)
  * [x] V√©rifier mapping ‚Üí `StructuredSegment[]`, d√©tection langue, erreurs 5xx

* [x] **Cr√©er** `tests/unit/search/normalizer.test.ts`

  * [x] V√©rifier `finalizeDocId` + d√©duplication (pas de doubles)
  * [x] S‚Äôassurer que les champs optionnels absents sont **omis** (pas `undefined`)

* [x] **Cr√©er** `tests/unit/search/ingest.toKnowledgeGraph.test.ts`

  * [x] Mocker `KnowledgeGraph` : v√©rifier **appel de `insert`** avec provenance, titre/langue/source
  * [x] V√©rifier g√©n√©ration `mentions` (stopwords FR/EN basiques)

* [x] **Cr√©er** `tests/unit/search/ingest.toVectorStore.test.ts`

  * [x] Mocker `VectorMemoryIndex` : v√©rifier **appel √† `upsert`** par chunk
  * [x] V√©rifier metadata (`docId,url,title,language,fetchedAt`)

* [x] **Cr√©er** `tests/unit/search/pipeline.test.ts`

  * [x] Cas **heureux** : 2‚Äì3 r√©sultats Searx ‚Üí 2 docs ing√©r√©s (KG + Vector)
  * [x] Cas **partiellement en √©chec** : 1 URL fail (erreur fetch/extract) ‚Üí event `search:error` √©mis, job continue
  * [x] V√©rifier **events** √©mis (voir section D)

### F2. Int√©gration (mocks r√©seaux, sans containers)

* [x] **Cr√©er** `tests/integration/search.tools.test.ts`

  * [x] Mocker SearxNG/Unstructured (nock)
  * [x] Appeler tool `search.run` ‚Üí v√©rifier **sortie tool**, appels KG/Vector, events

### F3. E2E (avec containers)

* [x] **Cr√©er** `tests/e2e/search.e2e.test.ts`

  * [x] D√©marrer `docker/docker-compose.search.yml` (ou script existant `scripts/run-http-e2e.mjs` adapt√©)
  * [x] Appeler `search.run` avec requ√™te de d√©mo (ex : `benchmarks LLM multimodal 2025 filetype:pdf`)
  * [x] Attendre compl√©tion ‚Üí v√©rifier :

    * [x] au moins 1 triple `Document` dans **KnowledgeGraph** (via tool/query existant)
    * [x] au moins 1 **VectorMemory** `upsert` (via tool de recherche m√©moire existant)
    * [x] Dashboard/endpoint stats expose un **compteur de jobs** et **latences**

### F4. Couverture & CI

* [x] Configurer la **couverture** (collecte existante OK) : viser ‚â•90% sur `src/search/**`
* [x] **CI** : ajouter un job qui monte `docker-compose.search.yml`, attend les healthchecks, lance `npm run test:e2e` et publie logs/coverage

---

## G) Documentation & Changelog

* [x] **Cr√©er** `docs/search-module.md`

  * [x] Architecture (sch√©mas, √©tapes du pipeline)
  * [x] Variables d‚Äôenvironnement et valeurs par d√©faut
  * [x] Exemples d‚Äôutilisation des tools `search.run` / `search.index`
  * [x] Strat√©gie de recrawl (ETag/Last-Modified, TTL par domaine)
  * [x] Limites connues (PDF scann√©s, OCR lourd, multilingue)
  * [x] Runbook incident (SearxNG down, Unstructured slow/5xx)

* [x] **Modifier** `CHANGELOG.md`

  * [x] Entr√©e : `feature: search llm searxng (module + tools + infra + tests)`

---

## H) Nettoyage et v√©rifications finales

* [x] **Aucun** `console.log` brut : utiliser `StructuredLogger`
* [x] **Aucune** cl√© optionnelle exposant `undefined` dans JSON publics
* [x] **Budgets** tools `search.*` revus (temps, toolCalls, bytesOut) selon perfs mesur√©es
* [x] **Smoke test** local : `docker compose -f docker/docker-compose.search.yml up -d` ‚Üí `search.run` ‚Üí dashboard OK

---

## üìÅ R√©cap **fichier par fichier** (cr√©ations/modifs)

**√Ä cr√©er :**

* [x] `src/search/types.ts` ‚Äî types pivots structur√©s
* [x] `src/search/config.ts` ‚Äî configuration centralis√©e du module
* [x] `src/search/searxClient.ts` ‚Äî client SearxNG (zod, timeouts, retries)
* [x] `src/search/downloader.ts` ‚Äî fetch robuste + `computeDocId`
* [x] `src/search/extractor.ts` ‚Äî client Unstructured + mapping segments
* [x] `src/search/normalizer.ts` ‚Äî finalisation d‚ÄôID + d√©dup
* [x] `src/search/ingest/toKnowledgeGraph.ts` ‚Äî **`KnowledgeGraph.insert`** + provenance + mentions
* [x] `src/search/ingest/toVectorStore.ts` ‚Äî **`VectorMemoryIndex.upsert`** + metadata
* [x] `src/search/pipeline.ts` ‚Äî orchestration end-to-end + events
* [x] `src/search/metrics.ts` ‚Äî timers/histogrammes
* [x] `src/search/index.ts` ‚Äî exports module
* [x] `src/tools/search_run.ts` ‚Äî tool principal (ingestion + retour docs)
* [x] `src/tools/search_index.ts` ‚Äî ingestion directe d‚ÄôURL(s)
* [x] `src/tools/search_status.ts` ‚Äî statut job (ou ‚Äúnon impl√©ment√©‚Äù propre)
* [x] `docker/docker-compose.search.yml` ‚Äî infra E2E
* [x] `docker/searxng/settings.yml` ‚Äî config engines/cat√©gories
* [x] `docs/search-module.md` ‚Äî runbook
* [x] `tests/unit/search/*.test.ts` ‚Äî 7 fichiers (client, downloader, extractor, normalizer, ingest KG, ingest Vector, pipeline)
* [x] `tests/integration/search.tools.test.ts` ‚Äî int√©gration tools search
* [x] `tests/e2e/search.e2e.test.ts` ‚Äî E2E containers

**√Ä modifier :**

* [x] `src/mcp/registry.ts` ‚Äî enregistrement des tools `search.*` (pack/tags/budgets/descriptions)
* [x] `src/monitor/dashboard.ts` ‚Äî panneau Search + endpoints JSON correspondants
* [x] `src/orchestrator/runtime.ts` ‚Äî wiring propre (si injection n√©cessaire du logger/tracing vers le pipeline)
* [x] `src/eventStore.ts` ‚Äî **optionnel** : si tu d√©cides d‚Äôajouter `EventKind = "SEARCH"`, √©tendre l‚Äôunion et adapter usages
* [x] `env/.env.example` ‚Äî ajouter toutes les variables `SEARCH_*` et `UNSTRUCTURED_*`
* [x] `CHANGELOG.md` ‚Äî nouvelle entr√©e

---

## üß™ Ce qu‚Äôil faut savoir et respecter concernant **les tests** et le **build**

* **Build** :

  * [ ] `npm run build` doit compiler sans warnings ; ESM strict ; imports Node en `node:` ; `graph-forge` unaffected.
  * [ ] Respecter **`exactOptionalPropertyTypes`** : jamais `undefined` dans les objets publics ; pr√©f√©rer omettre la cl√©.

* **Tests** :

  * [ ] **Unitaires** : isoler chaque fichier logique du module ; utiliser **nock** pour mocks HTTP ; ne pas d√©passer des timeouts √©lev√©s.
  * [ ] **Int√©gration** : valider le flow MCP tool ‚Üí pipeline ‚Üí KG/Vector ; v√©rifier les **events**.
  * [ ] **E2E** : monter les containers (`docker-compose.search.yml`), attendre **healthchecks**, lancer `npm run test:e2e`, v√©rifier ingestion r√©elle (au moins 1 doc).
  * [ ] **Couverture** : `src/search/**` ‚â• 90%, global ‚â• 85% ; √©viter tests flaky (retries c√¥t√© mocks plut√¥t que sleep).

* **Observabilit√©** :

  * [ ] Tracer les dur√©es par √©tape, compter les erreurs par type (Network, Unstructured, Ingest).
  * [ ] S‚Äôassurer que le **Dashboard** expose des stats consultables (JSON + page), et que les **SSE** restent stables.

---

Si tu coches ces cases dans l‚Äôordre, tu livreras un **moteur de recherche LLM** complet, **self-host** (SearxNG), **multimodal via unstructured**, solidement **int√©gr√©** au KnowledgeGraph et √† la VectorMemory du repo, avec **outillage MCP**, **observabilit√©**, et **tests** robustes.
## Historique Agent


### 2025-10-28
- V√©rification de l‚Äôimpl√©mentation compl√®te du module search, mise √† jour de la checklist et ex√©cution de `npm run test:unit` (√©choue ici sur `graph worker pool performance offloads heavy change-sets via worker threads when available`, `server module loader falls back to the compiled output when forced` et une sonde maintenance HTTP).

### 2025-11-18
- R√©initialisation du fichier AGENTS.md avec la nouvelle checklist fournie par l'utilisateur.
