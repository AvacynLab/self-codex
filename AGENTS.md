Parfait â€” voici une **liste de tÃ¢ches Ã  cocher**, prÃªte Ã  transmettre Ã  lâ€™agent, pour tester **les outils MCP**, le **fonctionnement global** (enfants, graphes, planification, Ã©vÃ©nements), **relever ce qui marche/ne marche pas**, et **proposer des amÃ©liorations**.
Adresse-toi au serveur via HTTP (port et chemin configurÃ©s), consigne tout dans un dossier de run isolÃ©.

---

# âœ… To-do de validation & amÃ©lioration â€” Self-Codex (Ã  lâ€™attention de lâ€™agent)

> Dossier de travail (Ã  crÃ©er au dÃ©but et Ã  rÃ©utiliser partout) : `runs/validation_<DATE-ISO>/`
> Arborescence attendue :
>
> ```
> runs/validation_<DATE-ISO>/
>   inputs/          # requÃªtes envoyÃ©es (1 fichier .jsonl par phase)
>   outputs/         # rÃ©ponses complÃ¨tes (1 fichier .jsonl par phase)
>   events/          # flux dâ€™Ã©vÃ©nements .jsonl (par phase)
>   logs/            # copies /tmp/mcp_http.log + rÃ©sumÃ©s
>   artifacts/       # artefacts produits par les outils (fichiers, graphes exportÃ©sâ€¦)
>   report/          # findings.json, summary.md, recommendations.md
> ```

## 0) PrÃ©flight & environnement

* [x] CrÃ©er `runs/validation_<DATE-ISO>/{inputs,outputs,events,logs,artifacts,report}`.
* [ ] Lire `process.env` utiles (HOST, PORT, PATH, TOKEN) et afficher la cible finale.
* [ ] VÃ©rifier accessibilitÃ© HTTP du MCP :

  * [ ] **Sans** Authorization â†’ obtenir **401**.
  * [ ] **Avec** `Authorization: Bearer <TOKEN>` â†’ **200** (health/info).
* [ ] Sauvegarder lâ€™en-tÃªte de session (URL, headers, date) dans `report/context.json`.

## 1) Introspection du serveur MCP

* [ ] `mcp_info` â†’ sauvegarder request/response dans `inputs/01_introspection.jsonl` & `outputs/01_introspection.jsonl`.
* [ ] `mcp_capabilities` â†’ lister transports, streaming, limites.
* [ ] `tools_list` â†’ inventaire des outils (noms, schÃ©mas dâ€™input/output).
* [ ] `resources_list` â†’ ressources disponibles (si applicable).
* [ ] `events_subscribe` â†’ ouvrir un flux sur `events/01_bus.jsonl`, vÃ©rifier numÃ©rotation/horodatage.

## 2) Journalisation & santÃ©

* [ ] Forcer lâ€™Ã©criture de logs via un outil simple (ex : ping/echo) ; vÃ©rifier que `/tmp/mcp_http.log` bouge.
* [ ] Copier `/tmp/mcp_http.log` â†’ `runs/validation_â€¦/logs/mcp_http.log`.
* [ ] Ajouter un rÃ©sumÃ© JSON (`logs/summary.json`) : lignes totales, erreurs, WARN, p95 latence (si Ã©valuable).

## 3) Transactions & graphes

* [ ] Cas nominal :

  * [ ] `tx_begin` â†’ rÃ©cupÃ©rer lâ€™ID de transaction.
  * [ ] `graph_diff` (baseline) â†’ attendre â€œno changesâ€.
  * [ ] `graph_patch` (ajout de nÅ“uds/edges) â†’ succÃ¨s.
  * [ ] `tx_commit` â†’ succÃ¨s.
* [ ] Cas erreur :

  * [ ] `tx_begin` â†’ `graph_patch` invalide (contrainte cassÃ©e) â†’ **erreur attendue**.
  * [ ] `tx_rollback` â†’ vÃ©rif que lâ€™Ã©tat nâ€™a pas changÃ©.
* [ ] Concurrence :

  * [ ] `graph_lock` (session A) puis tentative de modification par session B â†’ blocage explicite.
* [ ] Export :

  * [ ] `values_graph_export` / `causal_export` â†’ dÃ©poser fichiers dans `artifacts/graphs/`.
* [ ] Sauvegarder toutes les requÃªtes/rÃ©ponses dans `inputs/02_tx.jsonl` & `outputs/02_tx.jsonl`; Ã©vÃ©nements dans `events/02_tx.jsonl`.

## 4) Outils â€œgraph forge / analyseâ€

* [ ] `graph_forge_analyze` (sur un graphe jouet) â†’ vÃ©rifier qualitÃ© des diagnostics, formats de sortie.
* [ ] `graph_state_autosave start` â†’ attendre 2 cycles, vÃ©rifier Ã©vÃ©nements `autosave.tick`.
* [ ] `graph_state_autosave stop` â†’ plus de tick.
* [ ] Sauvegarder artefacts dâ€™analyse dans `artifacts/forge/`.

## 5) Enfants (instances Codex â€œself-forkâ€)

* [ ] `child_spawn_codex` â†’ crÃ©er un enfant avec mÃ©tadonnÃ©es (objectif, limites CPU/Mem/Wall).
* [ ] `child_attach` â†’ confirmer attachement & canaux de communication.
* [ ] `child_set_limits` â†’ rÃ©duire les quotas, tenter une tÃ¢che dÃ©passant le budget â†’ attendre lâ€™Ã©vÃ©nement `child.limit.exceeded`.
* [ ] `child_send` (prompt de test) â†’ attendre retour textuel + Ã©vÃ©nements de progression.
* [ ] `child_kill` / arrÃªt propre ; vÃ©rifier libÃ©ration des descripteurs/handles.
* [ ] Journaliser dans `inputs/05_children.jsonl`, `outputs/05_children.jsonl`, `events/05_children.jsonl`.

## 6) Planification & exÃ©cution (BT / rÃ©actif)

* [ ] `plan_compile_bt` sur un arbre simple (3 nÅ“uds : collecte â†’ transformation â†’ Ã©criture).
* [ ] `plan_run_bt` â†’ vÃ©rifier succession des Ã©tats RUNNING/SUCCESS/FAILURE.
* [ ] `plan_run_reactive` â†’ injecter un Ã©vÃ©nement externe et vÃ©rifier adaptation.
* [ ] `plan_pause` â†’ Ã©tat PAUSED, pas dâ€™avancement â†’ `plan_resume`.
* [ ] `plan_cancel` ou `op_cancel` sur tÃ¢che longue â†’ vÃ©rifier annulation propre.
* [ ] Exporter le journal du plan (si dispo) dans `artifacts/plans/`.

## 7) Coordination multi-agent

* [ ] Blackboard : `bb_set`, `bb_get`, `bb_query`, `bb_watch` (watch â†’ flux fini de 3-5 Ã©vÃ©nements).
* [ ] Stigmergie : `stig_mark` (avec intensitÃ©/dÃ©croissance), `stig_decay`, `stig_snapshot`.
* [ ] Contract-Net : `cnp_announce` â†’ collecter 2 propositions simulÃ©es, choisir la meilleure, notifier la dÃ©cision.
* [ ] Consensus : `consensus_vote` (pairage) â†’ vÃ©rifier tie-break stable, rÃ©sultat dÃ©terministe.
* [ ] Enregistrer tout dans `inputs/07_coord.jsonl`, `outputs/07_coord.jsonl`, `events/07_coord.jsonl`.

## 8) Connaissance & valeurs

* [ ] `kg_assist` / `kg_suggest_plan` â†’ vÃ©rifier qualitÃ© des suggestions (critÃ¨res : cohÃ©rence, citations internes si prÃ©vues).
* [ ] `kg_get_subgraph` â†’ cohÃ©rence structurale (noeuds/relations attendus).
* [ ] `values_explain` â†’ justification comprÃ©hensible, stable avec mÃªmes inputs (tester 2 runs identiques).
* [ ] `values_graph_export` / `causal_export` â†’ formats bien formÃ©s ; dÃ©poser dans `artifacts/knowledge/`.

## 9) Robustesse & erreurs contrÃ´lÃ©es

* [ ] Appels avec schÃ©ma dâ€™input invalide â†’ **400**/erreur outillÃ©e avec message utile.
* [ ] Outil inconnu â†’ **404** clair.
* [ ] Idempotency : mÃªme `Idempotency-Key` sur `tx_begin` â†’ mÃªme rÃ©ponse (sans duplicata).
* [ ] Crash simulÃ© de lâ€™enfant â†’ Ã©vÃ©nement dâ€™erreur + nettoyage (pas de fuite de process).
* [ ] Timeout volontaire (op longue + limite serrÃ©e) â†’ statut `cancelled`/`timeout` attendu.

## 10) Performance (sanity)

* [ ] Mesurer latence p50/p95/p99 dâ€™un outil lÃ©ger (ex: echo) sur 50 appels ; exporter `report/perf_summary.json`.
* [ ] Concurrence : lancer 5 tÃ¢ches enfants simultanÃ©es (charges faibles) â†’ vÃ©rifier stabilitÃ©/ordre des Ã©vÃ©nements.
* [ ] Taille des logs : rotation si configurÃ©e ; sinon, fichier unique mais non explosif.

## 11) SÃ©curitÃ© & redaction

* [ ] Tester masquage (`MCP_LOG_REDACT`) avec un faux secret dans lâ€™input â†’ sâ€™assurer quâ€™il nâ€™apparaÃ®t pas en clair.
* [ ] VÃ©rifier refus dâ€™accÃ¨s sans `Authorization` si le token est requis.
* [ ] VÃ©rifier absence de chemin dâ€™Ã©criture hors `runs/` pour les artefacts (pas dâ€™escape).

## 12) Rapport final & recommandations

* [ ] GÃ©nÃ©rer `report/findings.json` :

  * [ ] versions (node/npm, app, SDK)
  * [ ] outils testÃ©s (succÃ¨s/Ã©checs, latence p95)
  * [ ] incidents (erreurs, timeouts, violations de schÃ©ma)
  * [ ] KPIs : #events, ordonnancement (seq monotone), taille des artefacts
* [ ] GÃ©nÃ©rer `report/summary.md` (lisible) :

  * [ ] ce qui marche, ce qui ne marche pas
  * [ ] points ambigus Ã  clarifier cÃ´tÃ© outils/contrats dâ€™I/O
  * [ ] check des objectifs par phase (rÃ©ussi/partiel/Ã©chec)
* [ ] GÃ©nÃ©rer `report/recommendations.md` (priorisÃ© P0/P1/P2) :

  * [ ] P0 = corrections indispensables pour stabilitÃ© (ex : messages dâ€™erreur, validations dâ€™input, quotas enfants)
  * [ ] P1 = amÃ©liorations UX/dev (ex : schÃ©mas/outils plus stricts, docs, exemples)
  * [ ] P2 = optimisations (perf, logs, observabilitÃ©)

---

## Notes dâ€™exÃ©cution (conseils pratiques)

* Sauvegarde **chaque requÃªte** envoyÃ©e (JSON) et la **rÃ©ponse complÃ¨te** (JSON) dans les fichiers `.jsonl` des phases.
* Pour les Ã©vÃ©nements, utilise un **flux append** `.jsonl` par phase ; assure lâ€™ordre strictement croissant dâ€™un champ `seq` sâ€™il existe.
* Tout artefact (export de graphe, fichiers gÃ©nÃ©rÃ©s par les outils, journaux de plan) doit aller dans `artifacts/` avec un nom **horodatÃ©**.
* En cas dâ€™Ã©chec Ã  une Ã©tape, **note la cause exacte** (message dâ€™erreur, code, contexte) et propose une **action corrective** dans `recommendations.md`.

---

## Grille de sortie minimale (doit figurer dans le rapport)

* âœ… Liste des outils testÃ©s + statut (OK/KO/partiel)
* â±ï¸ Latences p50/p95/p99 (au moins sur 1 outil trivial + 1 outil lourd)
* ğŸ” Tests dâ€™idempotence et de concurrence documentÃ©s
* ğŸ§’ Gestion des enfants (spawn, limites, kill) validÃ©e / non validÃ©e
* ğŸ“Š Exports de graphes prÃ©sents & lisibles
* ğŸ§° Incidents & root causes (si incidents) + quick-fix proposÃ©s

---

Si tu veux, je peux aussi te fournir un **jeu de requÃªtes JSON dâ€™exemple** (un par phase) prÃªt Ã  Ãªtre injectÃ©, et un petit script Node pour router les appels et journaliser automatiquement dans lâ€™arborescence `runs/validation_<DATE-ISO>/`.

---
